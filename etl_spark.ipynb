{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e00745f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "# Step 0: Activate findspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Step 1: Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_utc_timestamp, col, from_unixtime, date_format, hour\n",
    "from pyspark.sql.functions import isnan, when, count, col, sum\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import concat_ws\n",
    "from pyspark.sql.functions import sha2, concat_ws\n",
    "from pyspark.sql.functions import input_file_name\n",
    "from datetime import datetime\n",
    "from pyspark.sql import Row\n",
    "import logging\n",
    "import uuid\n",
    "import glob\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f103e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_columns(df, expected_columns):\n",
    "    \n",
    "    \"\"\" \n",
    "    Check for missing columns in the DataFrame.\n",
    "    Args:\n",
    "        df (DataFrame): The Spark DataFrame to check.\n",
    "        expected_columns (list): List of expected column names.\n",
    "        \n",
    "        returns: list: List of missing column names.\n",
    "    \"\"\"\n",
    "    \n",
    "    # missing columns check\n",
    "    missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "    return missing_columns\n",
    "\n",
    "def check_null_columns(df):\n",
    "    \"\"\"\n",
    "    Check for null values in each column of the DataFrame.\n",
    "    Args:\n",
    "        df (DataFrame): The Spark DataFrame to check.\n",
    "    Returns:\n",
    "        dict: A dictionary with column names as keys and counts of null values as values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Count nulls per column\n",
    "    null_counts = df.select([\n",
    "        sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "    ])\n",
    "\n",
    "    # Step 2: Convert to dictionary\n",
    "    null_dict = null_counts.collect()[0].asDict()\n",
    "\n",
    "    # # Step 3: Loop and print only columns with missing values\n",
    "    # for column, count in null_dict.items():\n",
    "    #     if count > 0:\n",
    "    #         print(f\"Column '{column}' has {count} missing values.\")\n",
    "    return null_dict\n",
    "\n",
    "def check_duplicates(df):\n",
    "    \"\"\"\n",
    "    Check for duplicate rows in the DataFrame.\n",
    "    Args:\n",
    "        df (DataFrame): The Spark DataFrame to check.\n",
    "    Returns:\n",
    "        int: The count of duplicate rows.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Step 1: Concatenate all columns into one string\n",
    "    df_combined = df.withColumn(\"row_concat\", concat_ws(\"||\", *df.columns))\n",
    "\n",
    "    # Step 2: Apply SHA-256 hash to the concatenated string\n",
    "    df_hashed = df_combined.withColumn(\"row_sha256\", sha2(col(\"row_concat\"), 256))\n",
    "\n",
    "    # step 3 : show duplicates based on hash\n",
    "    result = df_hashed.groupBy(\"row_sha256\").count().filter(col(\"count\") > 1).count()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Define log schema\n",
    "def create_log(spark, uuid, ts_start, ts_end, process_name, sub_process_name ,table_destination, record_insert, column_length=None, log_table=\"spark_catalog.db.process_logs\"):\n",
    "    \"\"\" Create a log entry as a Row object.\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session object.\n",
    "        uuid (str): Unique identifier for the process.\n",
    "        ts_start (datetime): Start timestamp of the process.\n",
    "        ts_end (datetime): End timestamp of the process.\n",
    "        process_name (str): Name of the process.\n",
    "        sub_process_name (str): Name of the sub-process.\n",
    "        table_destination (str): Destination table name.\n",
    "        record_insert (int): Number of records inserted.\n",
    "        column_length (int): Number of columns processed.\n",
    "    Returns:\n",
    "        Row: A Row object containing the log entry.\n",
    "    \"\"\"\n",
    "    \n",
    "    msg = Row(\n",
    "        # timestamp=datetime.now().isoformat(),\n",
    "        uuid=uuid,\n",
    "        ts_start=ts_start,\n",
    "        ts_end=ts_end,\n",
    "        duration_seconds=(ts_end - ts_start).total_seconds(),\n",
    "        process_name=process_name,\n",
    "        sub_process_name=sub_process_name,\n",
    "        table_destination=table_destination,\n",
    "        record_insert=record_insert,\n",
    "        column_length=column_length\n",
    "    )\n",
    "    \n",
    "    log_entry = [msg]\n",
    "    log_df = spark.createDataFrame(log_entry)\n",
    "    log_df.writeTo(f\"{log_table}\").append()\n",
    "    return msg\n",
    "\n",
    "def create_error_log(spark, uuid, process_name, sub_process_name ,table_destination, error_message, log_table=\"spark_catalog.db.error_logs\"):\n",
    "    \"\"\" Create an error log entry as a Row object.\n",
    "    Args:\n",
    "        spark (SparkSession): The Spark session object.\n",
    "        process_name (str): Name of the process.\n",
    "        sub_process_name (str): Name of the sub-process.\n",
    "        table_destination (str): Destination table name.\n",
    "        error_message (str): Error message.\n",
    "    Returns:\n",
    "        Row: A Row object containing the error log entry.\n",
    "    \"\"\"\n",
    "    msg = Row(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        uuid=uuid,\n",
    "        process_name=process_name,\n",
    "        sub_process_name=sub_process_name,\n",
    "        table_destination=table_destination,\n",
    "        error_message=error_message\n",
    "    )\n",
    "\n",
    "    error_log_entry = [msg]\n",
    "    error_log_df = spark.createDataFrame(error_log_entry)\n",
    "    error_log_df.writeTo(f\"{log_table}\").append()\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c9335d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 10:56:30,459 INFO spark_logger: Process UUID: 37b9a4fa-8ee4-49e6-b79e-df54a8f8eefb\n"
     ]
    }
   ],
   "source": [
    "# Configure logging to file\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(name)s: %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"spark_job.log\"),  # Log file path\n",
    "        logging.StreamHandler()               # Also log to console\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"spark_logger\")\n",
    "\n",
    "# set uuid     \n",
    "uuid_ = str(uuid.uuid4())\n",
    "\n",
    "logger.info(f\"Process UUID: {uuid_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b9b8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 10:56:51,513 INFO spark_logger: load yaml file\n",
      "2025-11-15 10:56:51,513 INFO spark_logger: raw_data_path: d:\\works\\Python\\Met_DE\\Sales_data_1\\sales_data\\\n",
      "2025-11-15 10:56:51,515 INFO spark_logger: clean_data_path: d:\\works\\Python\\Met_DE\\cleansed\\\n",
      "2025-11-15 10:56:51,515 INFO spark_logger: iceberg_jar: C:\\spark\\spark-3.3.2-bin-hadoop3\\jars\\iceberg-spark-runtime-3.3_2.12-1.3.0.jar\n",
      "2025-11-15 10:56:51,517 INFO spark_logger: warehouse_path: d:\\works\\Python\\Met_DE\\iceberg\n",
      "2025-11-15 10:56:51,517 INFO spark_logger: iceberg_schema: spark_catalog.db\n",
      "2025-11-15 10:56:51,518 INFO spark_logger: logs_table: spark_catalog.db.process_logs\n",
      "2025-11-15 10:56:51,520 INFO spark_logger: error_table: spark_catalog.db.error_logs\n"
     ]
    }
   ],
   "source": [
    "# Load YAML from a file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    logger.info(\"load yaml file\")\n",
    "\n",
    "\n",
    "project_path = os.getcwd()\n",
    "\n",
    "raw_data_path = os.path.join(project_path, config['etl']['raw_data_path']) \n",
    "clean_data_path = os.path.join(project_path, config['etl']['cleansed_data_path'])\n",
    "iceberg_jar = config['iceberg']['jar']\n",
    "warehouse_path = os.path.join(project_path, config['iceberg']['warehouse_location'])\n",
    "iceberg_schema = config.get('iceberg').get('catalog_name') + \".\" + config.get('iceberg').get('database_name') \n",
    "logs_table = iceberg_schema + \".\" + config.get('iceberg').get('logs_table')\n",
    "error_table = iceberg_schema + \".\" + config.get('iceberg').get('error_table') \n",
    "\n",
    "logger.info(f\"raw_data_path: {raw_data_path}\")\n",
    "logger.info(f\"clean_data_path: {clean_data_path}\")\n",
    "logger.info(f\"iceberg_jar: {iceberg_jar}\")\n",
    "logger.info(f\"warehouse_path: {warehouse_path}\")\n",
    "logger.info(f\"iceberg_schema: {iceberg_schema}\")\n",
    "logger.info(f\"logs_table: {logs_table}\")\n",
    "logger.info(f\"error_table: {error_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "796837df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 10:56:59,119 INFO spark_logger: Spark session created\n"
     ]
    }
   ],
   "source": [
    "# create spark session with iceberg and s3a configurations\n",
    "    \n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"spark_etl\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .config(\"spark.hadoop.io.nativeio.NativeIO.disable\", \"true\")\\\n",
    "#     .config(\"spark.hadoop.fs.use.nativeio\", \"false\")\\\n",
    "#     .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\\\n",
    "#     .config(\"spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version\", \"2\")\\\n",
    "#     .getOrCreate()\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spark_etl\") \\\n",
    "    .config(\"spark.jars\", iceberg_jar) \\\n",
    "    .config(\"spark.hadoop.hadoop.native.io\", \"false\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", warehouse_path) \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "# Reduce Spark internal logs\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "logger.info(\"Spark session created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9365a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"drop table if exists {}\".format(logs_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c53b3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 11:00:56,380 INFO spark_logger: Logs table ensured\n",
      "2025-11-15 11:00:56,399 INFO spark_logger: Error log table ensured\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    # create logs table and gold table if not exists\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {logs_table} (\n",
    "      uuid STRING,\n",
    "      ts_start STRING,\n",
    "      ts_end STRING,\n",
    "      duration_seconds DOUBLE,\n",
    "      process_name STRING,\n",
    "      sub_process_name STRING,\n",
    "      table_destination STRING,\n",
    "      record_insert INT,\n",
    "      column_length INT\n",
    "    )\n",
    "    USING iceberg\n",
    "    \"\"\")\n",
    "\n",
    "    logger.info(\"Logs table ensured\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating logs table: {str(e)}\")\n",
    "    # sys.exit()\n",
    "\n",
    "try : \n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {error_table} (\n",
    "        timestamp STRING,\n",
    "        uuid STRING,\n",
    "        process_name STRING,\n",
    "        sub_process_name STRING,\n",
    "        table_destination STRING,\n",
    "        error_message STRING\n",
    "    )\n",
    "    USING iceberg\n",
    "    \"\"\")\n",
    "\n",
    "    logger.info(\"Error log table ensured\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error creating error log table: {str(e)}\")\n",
    "    # sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54a7ff54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 11:03:02,104 INFO spark_logger: CSV file read from source\n"
     ]
    }
   ],
   "source": [
    "# Read CSV files from a folder or pattern\n",
    "try :\n",
    "    ts_start = datetime.now()\n",
    "    df = spark.read.option(\"header\", True).csv(raw_data_path + \"*.csv\")\n",
    "    ts_end = datetime.now()\n",
    "    logger.info(\"CSV file read from source\")\n",
    "    log_entry = create_log(spark,uuid_, ts_start, ts_end, \"ETL ADLS\", \"load csv from source\", \"spark dataframe\", df.count(), len(df.columns))\n",
    "except Exception as e:  \n",
    "    logger.error(f\"Error during reading CSV from source: {str(e)}\")\n",
    "    error_entry = create_error_log(spark,uuid_,\"ETL ADLS\", \"load csv from source\", raw_data_path, str(e))\n",
    "    # sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aece5f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add filename column\n",
    "# df = df.withColumn(\"source_file\", input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d251685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-15 11:03:42,538 INFO spark_logger: No missing columns found.\n",
      "2025-11-15 11:03:51,022 ERROR spark_logger: Null Check: {'Order ID': 545, 'Product': 545, 'Quantity Ordered': 545, 'Price Each': 545, 'Order Date': 545, 'Purchase Address': 545}\n",
      "2025-11-15 11:04:07,298 ERROR spark_logger: Duplicate Rows Found: 266\n"
     ]
    }
   ],
   "source": [
    "# data quality \n",
    "\n",
    "# check missing columns\n",
    "ts_start = datetime.now()\n",
    "missing_columns = check_missing_columns(df,config['expected_columns'])\n",
    "ts_end = datetime.now()\n",
    "\n",
    "if missing_columns:\n",
    "    # print(f\"Missing columns: {missing_columns}\")\n",
    "    logger.error(f\"Missing Columns: {missing_columns}\")\n",
    "    logger.error(\"send alert to the team and process stop\")\n",
    "    error_entry = create_error_log(spark,uuid_,\"ETL ADLS\", \"check missing_columns\", \"spark dataframe\", f\"Missing columns: {'|'.join(missing_columns)}\")\n",
    "    # sys.exit()\n",
    "else:\n",
    "    # print(\"No missing columns.\")\n",
    "    logger.info(\"No missing columns found.\")\n",
    "    log_entry = create_log(spark,uuid_, ts_start, ts_end, \"ETL ADLS\", \"check missing_columns\", \"spark dataframe\", df.count(), len(df.columns))\n",
    "\n",
    "# check null values\n",
    "ts_start = datetime.now()\n",
    "null_check = check_null_columns(df)\n",
    "ts_end = datetime.now()\n",
    "\n",
    "if [(k,v) for k, v in null_check.items() if v > 0] : \n",
    "    # logger.error(f\"Null Check: {null_check}\")\n",
    "    logger.error(f\"Null Check: {null_check}\")\n",
    "    error_entry = create_error_log(spark,uuid_,\"ETL ADLS\", \"check null_columns\", \"spark dataframe\", f\"Null columns: {'|'.join([k for k, v in null_check.items() if v > 0])}\")\n",
    "\n",
    "else :\n",
    "    # print(\"No missing values found.\")\n",
    "    logger.info(\"No missing values found.\")\n",
    "    log_entry = create_log(spark,uuid_, ts_start, ts_end, \"ETL ADLS\", \"check null_columns\", \"spark dataframe\", df.count(), len(df.columns))\n",
    "\n",
    "# check duplicate rows\n",
    "ts_start = datetime.now()\n",
    "duplicate_count = check_duplicates(df)\n",
    "ts_end = datetime.now()\n",
    "if duplicate_count > 0:\n",
    "    logger.error(f\"Duplicate Rows Found: {duplicate_count}\")\n",
    "    df = df.dropDuplicates()\n",
    "    error_entry = create_error_log(spark,uuid_,\"ETL ADLS\", \"check duplicates\", \"spark dataframe\", f\"Duplicate rows found: {duplicate_count}\")\n",
    "else:\n",
    "    logger.info(\"No duplicate rows found.\")\n",
    "    log_entry = create_log(spark,uuid_, ts_start, ts_end, \"ETL ADLS\", \"remove duplicate\", \"spark dataframe\", df.count()-duplicate_count, len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8245504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transformation \n",
    "\n",
    "try :\n",
    "    # casting order date data types to from string to datetype  \n",
    "    ts_start = datetime.now()\n",
    "    df = df.withColumn(\n",
    "        \"Order Date New\",\n",
    "        F.to_timestamp(\"Order Date\", \"MM/dd/yy HH:mm\")\n",
    "    )\n",
    "    ts_end = datetime.now()\n",
    "    log_entry = create_log(spark,uuid_, ts_start, ts_end, \"ETL ADLS\", \"cast order date\", \"spark dataframe\", df.count(), len(df.columns))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during casting order date: {str(e)}\")\n",
    "    error_entry = create_error_log(spark,uuid_,\"ETL ADLS\", \"cast order date\", \"spark dataframe\", str(e))\n",
    "    # sys.exit()\n",
    "\n",
    "# convert to date only (without time component)\n",
    "# df = df.withColumn(\n",
    "#     \"order_date_only\",\n",
    "#     F.to_date(\"Order Date New\", \"MM/dd/yy HH:mm\")\n",
    "# )\n",
    "try :\n",
    "    ts_start = datetime.now()\n",
    "    df = df.withColumn(\"report_month\", F.date_format(F.col(\"Order Date New\"), \"yyyy-MM\"))\n",
    "    ts_end = datetime.now()\n",
    "    log_entry = create_log(spark,uuid_, ts_start, ts_end, \"ETL ADLS\", \"convert to date only\", \"spark dataframe\", df.count(), len(df.columns))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during converting to date only: {str(e)}\")\n",
    "    error_entry = create_error_log(spark,uuid_,\"ETL ADLS\", \"convert to date only\", \"spark dataframe\", str(e))\n",
    "    # sys.exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b96c0092",
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    # write to parquet partitioned by report_month\n",
    "    ts_start = datetime.now()\n",
    "    df.write \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .partitionBy(\"report_month\") \\\n",
    "      .parquet(clean_data_path)\n",
    "    ts_end = datetime.now()\n",
    "    log_entry = create_log(spark,uuid_, ts_start, ts_end, \"ETL ADLS\", \"write parquet\", \"spark parquet\", df.count(), len(df.columns))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during writing parquet: {str(e)}\")\n",
    "    error_entry = create_error_log(spark,uuid_,\"ETL ADLS\", \"write parquet\", \"spark parquet\", str(e))\n",
    "    # sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c4fcba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5b828b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
